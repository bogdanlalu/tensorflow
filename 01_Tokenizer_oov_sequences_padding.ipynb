{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01.Tokenizer_oov_sequences_padding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP9SgaLCPkAhMbg6ffKODCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogdanlalu/tensorflow/blob/master/01_Tokenizer_oov_sequences_padding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1q4c9poPqdJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1. Instantiate a Tokenizer object with a parameter to set a limit on top N words by frequency of appearence (`num_words`) and a custom token for out of vocabulary (`oov_token`) words. The out of vocabulary words are either those that fall below the `num_words` threshold or, if we are working with new text, words that have not been present when the tokenizer was fitted. Special characters defined using the `filters` parameter will be ignored.\n",
        "2. We fit the tokenizer to a list of sentences (corpus) using the `fit_on_texts` method. The resulting tokenizer will contain a `word_index` which is a dictionary where the keys are the unique words in the corpus and the values are the numeric ID those words have have been assigned.\n",
        "3. To encode the words as numbers we create *sequences* object, using the *tokenizer's* `texts_to_sequences` method and pass in the *entire corpus* like in step #2. Words that fall below `num_words` threshold are encoded with the numeric code assigned to the `oov_token`.\n",
        "4. To convert the resulting encoding back as text use the *tokenizer's* `sequences_to_texts` method and pass in the sequences object created at the previous step. Words that fell below the treshold will be replaced with the `oov_token` text.\n",
        "5. To account for difference in the length of sentences and create vectors of equal length, we use *padding*. This is a function which takes a sequences object as parameter, resulting in a new sequences object. The function fills gaps with 0, to make vectors of equal length. An optional  `maxlen` parameter can be used to restict all vectors (sentences) to a maximum length, resulting in truncating. Padding can be added *before* the words of each sentence using the `pre` value of the `padding` parameter (think of right aligning rows of text in Word and filling blanks with 0), or *after*, (think of left aligning rows of text in Word) using the `post` value for the `padding` parameter. Choosing pre or post padding impacts truncating. If pre padding is used, the `pre` value should also be chosen for `truncating` parameter. Conversely post truncating should be used with post padding. The logic behind this is to mostly truncate 0s.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV05Q5tXvK2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Uhnn93jwPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences1 = [\n",
        "             'I love my (dog)',\n",
        "             'I love my cat']\n",
        "sentences2 = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "              'You love my cat and dog']\n",
        "sentences3 = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "              'You love my cat',\n",
        "              \"I don't love you, and you don't love me, and my cat loves mice but doesn't love you!\"]\n",
        "all_sentences = [sentences1, sentences2, sentences3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JCmxFigQcfJ",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize, create sequences and convert them back to text, test on new sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fvf7W0QkIqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_tokenizer_proc_pipeline(text:List[str], n:int, new_text:List[str]):\n",
        "  tok = Tokenizer(num_words=n, oov_token='<OOV>')\n",
        "  tok.fit_on_texts(text)\n",
        "\n",
        "  sequences = tok.texts_to_sequences(text)\n",
        "  sequences_text = tok.sequences_to_texts(sequences)\n",
        "  print('-'*50)\n",
        "  print(\"word index: \", tok.word_index)\n",
        "  print(\"num words: \", tok.num_words)\n",
        "  \n",
        "  print(\"sequences:\", sequences)\n",
        "  print(\"seq2texts:\", sequences_text)\n",
        "  print('-'*50)\n",
        "\n",
        "  new_sequences = tok.texts_to_sequences(new_text)\n",
        "  new_sequences_text = tok.sequences_to_texts(new_sequences)\n",
        "  print(\"new_sequences:\", new_sequences)\n",
        "  print(\"new_seq2texts:\", new_sequences_text)\n",
        "\n",
        "  print(\"=\"*50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5SQO8guRAef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = ['My dog hates your cat.', \n",
        "                  'My cat likes fish and milk.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeRbo9O5lG8h",
        "colab_type": "code",
        "outputId": "1d5006d8-cdc2-4158-8c7b-67af853e3d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        }
      },
      "source": [
        "for sentence in all_sentences:\n",
        "  show_tokenizer_proc_pipeline(sentence, 8,test_sentences)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6}\n",
            "num words:  8\n",
            "sequences: [[2, 3, 4, 5], [2, 3, 4, 6]]\n",
            "padded_sequences [[2 3 4 5]\n",
            " [2 3 4 6]]\n",
            "padded_sequences2text: ['i love my dog', 'i love my cat']\n",
            "--------------------------------------------------\n",
            "new_sequences: [[4, 5, 1, 1, 6], [4, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[4 5 1 1 6 0]\n",
            " [4 6 1 1 1 1]]\n",
            "new_p_seq2texts: ['my dog <OOV> <OOV> cat <OOV>', 'my cat <OOV> <OOV> <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'and': 8}\n",
            "num words:  8\n",
            "sequences: [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 6, 1, 5]]\n",
            "padded_sequences [[4 2 3 5 0 0]\n",
            " [4 2 3 6 0 0]\n",
            " [7 2 3 6 1 5]]\n",
            "padded_sequences2text: ['i love my dog <OOV> <OOV>', 'i love my cat <OOV> <OOV>', 'you love my cat <OOV> dog']\n",
            "--------------------------------------------------\n",
            "new_sequences: [[3, 5, 1, 1, 6], [3, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[3 5 1 1 6 0]\n",
            " [3 6 1 1 1 1]]\n",
            "new_p_seq2texts: ['my dog <OOV> <OOV> cat <OOV>', 'my cat <OOV> <OOV> <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'love': 2, 'my': 3, 'you': 4, 'i': 5, 'cat': 6, \"don't\": 7, 'and': 8, 'dog': 9, 'me': 10, 'loves': 11, 'mice': 12, 'but': 13, \"doesn't\": 14}\n",
            "num words:  8\n",
            "sequences: [[5, 2, 3, 1], [5, 2, 3, 6], [4, 2, 3, 6], [5, 7, 2, 4, 1, 4, 7, 2, 1, 1, 3, 6, 1, 1, 1, 1, 2, 4]]\n",
            "padded_sequences [[5 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [5 2 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [4 2 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [5 7 2 4 1 4 7 2 1 1 3 6 1 1 1 1 2 4]]\n",
            "padded_sequences2text: ['i love my <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>', 'i love my cat <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>', 'you love my cat <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>', \"i don't love you <OOV> you don't love <OOV> <OOV> my cat <OOV> <OOV> <OOV> <OOV> love you\"]\n",
            "--------------------------------------------------\n",
            "new_sequences: [[3, 1, 1, 1, 6], [3, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[3 1 1 1 6 0]\n",
            " [3 6 1 1 1 1]]\n",
            "new_p_seq2texts: ['my <OOV> <OOV> <OOV> cat <OOV>', 'my cat <OOV> <OOV> <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g46xq0BuIedp",
        "colab_type": "text"
      },
      "source": [
        "# Add sequence padding to account for different lengths in sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hpxr9vUIpH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeCcVvPxIbxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_tokenizer_proc_pipeline(text: List[str], \n",
        "                                 n: int, \n",
        "                                 new_text: List[str],\n",
        "                                 length: int = None):\n",
        "  tok = Tokenizer(num_words=n, oov_token='<OOV>')\n",
        "  tok.fit_on_texts(text)\n",
        "\n",
        "  sequences = tok.texts_to_sequences(text)\n",
        "\n",
        "  padded_seq = pad_sequences(sequences, \n",
        "                             padding='post', \n",
        "                             truncating='post', \n",
        "                             maxlen=length)\n",
        "  \n",
        "  pad_sequences_text = tok.sequences_to_texts(padded_seq)\n",
        "\n",
        "  print('-'*50)\n",
        "  print(\"word index: \", tok.word_index)\n",
        "  print(\"num words: \", tok.num_words)\n",
        "  \n",
        "  print(\"sequences:\", sequences)\n",
        "  print(\"padded_sequences\", padded_seq)\n",
        "  print(\"padded_sequences2text:\", pad_sequences_text)\n",
        "\n",
        "  print('-'*50)\n",
        "\n",
        "  new_sequences = tok.texts_to_sequences(new_text)\n",
        "\n",
        "  new_paddded_seq = pad_sequences(new_sequences, \n",
        "                                  padding='post', \n",
        "                                  truncating='post', \n",
        "                                  maxlen=length)\n",
        "  \n",
        "  pad_new_sequences_text = tok.sequences_to_texts(new_paddded_seq)\n",
        "  \n",
        "  print(\"new_sequences:\", new_sequences)\n",
        "  print(\"new_padded_sequences:\", new_paddded_seq)\n",
        "  print(\"new_p_seq2texts:\", pad_new_sequences_text)\n",
        "  print(\"=\"*50)\n",
        "  print('\\n'*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmHo847GM3uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = ['My dog hates your cat.', \n",
        "                  'My cat likes fish and milk.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eikl1sbNHiwm",
        "colab_type": "code",
        "outputId": "612632a4-161a-4083-e0ce-8bbd480d0388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        }
      },
      "source": [
        "for sentence in all_sentences:\n",
        "  show_tokenizer_proc_pipeline(sentence, 8, test_sentences, length=4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'i': 2, 'love': 3, 'my': 4, 'dog': 5, 'cat': 6}\n",
            "num words:  8\n",
            "sequences: [[2, 3, 4, 5], [2, 3, 4, 6]]\n",
            "padded_sequences [[2 3 4 5]\n",
            " [2 3 4 6]]\n",
            "padded_sequences2text: ['i love my dog', 'i love my cat']\n",
            "--------------------------------------------------\n",
            "new_sequences: [[4, 5, 1, 1, 6], [4, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[4 5 1 1]\n",
            " [4 6 1 1]]\n",
            "new_p_seq2texts: ['my dog <OOV> <OOV>', 'my cat <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7, 'and': 8}\n",
            "num words:  8\n",
            "sequences: [[4, 2, 3, 5], [4, 2, 3, 6], [7, 2, 3, 6, 1, 5]]\n",
            "padded_sequences [[4 2 3 5]\n",
            " [4 2 3 6]\n",
            " [7 2 3 6]]\n",
            "padded_sequences2text: ['i love my dog', 'i love my cat', 'you love my cat']\n",
            "--------------------------------------------------\n",
            "new_sequences: [[3, 5, 1, 1, 6], [3, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[3 5 1 1]\n",
            " [3 6 1 1]]\n",
            "new_p_seq2texts: ['my dog <OOV> <OOV>', 'my cat <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "word index:  {'<OOV>': 1, 'love': 2, 'my': 3, 'you': 4, 'i': 5, 'cat': 6, \"don't\": 7, 'and': 8, 'dog': 9, 'me': 10, 'loves': 11, 'mice': 12, 'but': 13, \"doesn't\": 14}\n",
            "num words:  8\n",
            "sequences: [[5, 2, 3, 1], [5, 2, 3, 6], [4, 2, 3, 6], [5, 7, 2, 4, 1, 4, 7, 2, 1, 1, 3, 6, 1, 1, 1, 1, 2, 4]]\n",
            "padded_sequences [[5 2 3 1]\n",
            " [5 2 3 6]\n",
            " [4 2 3 6]\n",
            " [5 7 2 4]]\n",
            "padded_sequences2text: ['i love my <OOV>', 'i love my cat', 'you love my cat', \"i don't love you\"]\n",
            "--------------------------------------------------\n",
            "new_sequences: [[3, 1, 1, 1, 6], [3, 6, 1, 1, 1, 1]]\n",
            "new_padded_sequences: [[3 1 1 1]\n",
            " [3 6 1 1]]\n",
            "new_p_seq2texts: ['my <OOV> <OOV> <OOV>', 'my cat <OOV> <OOV>']\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}